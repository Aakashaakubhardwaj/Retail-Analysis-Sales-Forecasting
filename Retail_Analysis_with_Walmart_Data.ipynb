#Retail Analysis with Walmart Data


importing lybraries

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib import dates
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')
%matplotlib inline

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error


loading dataset as data

data = pd.read_csv("/content/Walmart_Store_sales.csv")

data.head()

checking null values

data.info()

data['Date'] =  pd.to_datetime(data['Date'], format="%d-%m-%Y")
data.info()

data.isnull().sum()

data["Day"]= pd.DatetimeIndex(data['Date']).day
data['Month'] = pd.DatetimeIndex(data['Date']).month
data['Year'] = pd.DatetimeIndex(data['Date']).year
data.head()

Total Sales for each store and finding the store which have maximum sales 

total_sales_by_store = data.groupby('Store')['Weekly_Sales'].sum().sort_values(ascending=0)

plt.figure(figsize = (15,7))
sns.barplot(data = data , y='Weekly_Sales',x= 'Store', order = total_sales_by_store.index,
            errorbar=('ci', False),palette='ocean')

plt.title('Total sales for each store', size = 20)
plt.xlabel('Store', size = 15)
plt.ylabel('Total Sales', size = 15)

total_sales_by_store_sum = pd.DataFrame(total_sales_by_store)
print('The store number',total_sales_by_store_sum.head(1).index[0],'has maximum sales with ${0:.2f} $'.format(total_sales_by_store_sum
                                     .head(1).Weekly_Sales[total_sales_by_store_sum.head(1).index[0]]))

checking which store has maximum standard deviation i.e., the sales vary a lot. Also, find out the coefficient of mean to standard deviation



data_std = pd.DataFrame(data.groupby("Store")['Weekly_Sales'].std().sort_values(ascending=0))
print('The store number',data_std.head(1).index[0],'has maximum standard deviation with ${0:.4f}'.format(data_std
                                     .head(1).Weekly_Sales[data_std.head(1).index[0]]))

coef_mean_std = pd.DataFrame(data.groupby('Store')['Weekly_Sales'].std() / data.groupby('Store')['Weekly_Sales'].mean())
coef_mean_std = coef_mean_std.rename(columns={'Weekly_Sales':'Coef. of mean to STD'})
coef_mean_std.sort_values(by='Coef. of mean to STD',ascending=0)

checking which store/s has good quarterly growth rate in Q3â€™2012



plt.figure(figsize=(15,7))

quarter_3 = data[(data['Date'] >= '2012-07-01') & (data['Date'] <= '2012-09-30')].groupby('Store')['Weekly_Sales'].sum()
sns.barplot(data = data , y='Weekly_Sales',x= 'Store', order = quarter_3.index)

quarter_3_store = pd.DataFrame(quarter_3.sort_values(ascending=0))
print('The store number',quarter_3_store.head(1).index[0],'has maximum sales during third quarter of year 2012 with ${0:.2f}'.format(quarter_3_store
                                     .head(1).Weekly_Sales[quarter_3_store.head(1).index[0]]))

super_bowl = data[data['Date'].isin(['2010-02-12','2011-02-11','2012-02-10','2013-02-08'])]
labour_day = data[data['Date'].isin(['2010-09-10','2011-09-09','2012-09-07','2013-09-06'])]
thanksgiving = data[data['Date'].isin(['2010-11-26','2011-11-25','2012-11-23','2013-11-29'])]
christmas = data[data['Date'].isin(['2010-12-31','2011-12-30','2012-12-28','2013-12-27'])]
no_holiday = data[data['Holiday_Flag'] == 0]

y = [super_bowl['Weekly_Sales'].mean(),
    labour_day['Weekly_Sales'].mean(),
    thanksgiving['Weekly_Sales'].mean(),
    christmas['Weekly_Sales'].mean(),
    no_holiday['Weekly_Sales'].mean()]

x = ['Super Bowl',
    'Labour Day',
    'Thanksgiving',
    'Christmas',
    'No Holiday']


sns.barplot(x = x,y = y,palette='plasma')

ThanksGiving has highest sales while Christmas has lower sales than average daily sales



data.head()

Providing a monthly and semester view of sales in units and give insights



mr_ws_2010 = data[(data['Year'] == 2010) | (data['Year'] == 2010)].groupby('Month')['Weekly_Sales'].sum() 
mr_ws_2011 = data[(data['Year'] == 2011) | (data['Year'] == 2011)].groupby('Month')['Weekly_Sales'].sum() 
mr_ws_2012 = data[(data['Year'] == 2012) | (data['Year'] == 2012)].groupby('Month')['Weekly_Sales'].sum()

fig, axes = plt.subplots(nrows= 1, ncols= 3, figsize = (13,4))

sns.barplot(data = mr_ws_2010, ax = axes[0])
sns.barplot(data = mr_ws_2011, ax = axes[1])
sns.barplot(data = mr_ws_2012, ax = axes[2])

axes[0].title.set_text("Monthly Sale 2010") 
axes[1].title.set_text("Monthly Sale 2011") 
axes[2].title.set_text("Monthly Sale 2012")



data['Semester'] = data.Date.dt.year.astype(str) + ' S'+ np.where(data.Date.dt.quarter.gt(2),2,1).astype(str)

plt.figure(figsize=(6, 5))
ax = sns.barplot(data = data.drop('Date',
                           axis = 1).groupby('Semester').sum().reset_index(),
            x = 'Semester',
            y = 'Weekly_Sales',
            palette = 'cool')

for i, bar in enumerate(ax.patches):
    plt.annotate(str(bar.get_height()),
                 xy=(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5),
                 ha='left',
                 va='top',
                 rotation = 90)
    
plt.title("Semesterly Sales")
plt.ylabel("Sales")

plt.show()

data.head()

Semester = data.groupby(pd.Grouper(key='Date', freq='6M')).sum()
Semester = Semester.reset_index()
fig, ax = plt.subplots(figsize=(13,4), dpi = 80)
X = Semester['Date']
Y = Semester['Weekly_Sales']
plt.plot(X,Y)
plt.title('Semester Wise Sales')
plt.xlabel('Semester')
plt.ylabel('Weekly Sales')
plt.legend('Sales')
plt.show()

We can infer that there's a big spike in sales from February-2010 to February-2011. Exactly for one year we can say.
Then spike goes bit down in February-2011 after that again there are few ups-downs in further.
From August-2012 sales goes down. - We can acknowledge that there is loss in sales.

plt.figure(figsize=(10,6))
data.groupby("Year")[["Weekly_Sales"]].sum().plot(kind='bar',legend=False)
ax1 = sns.barplot(data = data.drop('Date',
                           axis = 1).groupby('Year').sum().reset_index(),
            x = 'Year',
            y = 'Weekly_Sales',
            palette = 'cool')
for i, bar in enumerate(ax1.patches):
    plt.annotate(str(bar.get_height()),
                 xy=(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5),
                 ha='left',
                 va='top',
                 rotation = 90 , size =15)
plt.xlabel("Years")
plt.ylabel("Weekly Sales")
plt.title("Yearly view of sales")

fig, axes = plt.subplots(nrows= 1, ncols= 4, figsize = (14,3))
aa = data[['Temperature','Fuel_Price','CPI','Unemployment']]
for i,column in enumerate(aa):
    sns.boxplot(data[column], ax=axes[i])

data_new = data[(data['Unemployment']<11) & (data['Unemployment']>4.5) & (data['Temperature']>10)]
data_new.shape

data_new.columns

df = data_new[data_new.Store==1]
df.head()

df_corr = df.drop(['Date','Semester','Store','Year','Month','Holiday_Flag'],axis=1).corr()
df_corr

plt.figure(figsize=(12,6))
sns.heatmap(df_corr,annot=True)

Hypothesize if CPI, unemployment, and fuel price have any impact on sales.

Insights :

As we can see unemployment is highly correlated with days and is insignificant as it correlation with Weekly Sales is quite low.

Also temperature and Unemployment are negatively impacting the sales .

However Fuel Price and CPI are positively impacting the Sales.

df['Dummy_Date'] = np.arange(1, 144)
df.head()

X = df[['Day', 'Fuel_Price', 'CPI', 'Unemployment','Dummy_Date']]
Y = df['Weekly_Sales']

X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2,random_state=123)

model = LinearRegression()
model.fit(X_train,Y_train)

y_pred = model.predict(X_test)
y_pred

from sklearn import metrics

print('Accuracy:',model.score(X_train,Y_train))
print('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, y_pred))
print('Mean Squared Error:', metrics.mean_squared_error(Y_test, y_pred))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred)))

model.coef_

print('Variance score: %.2f' % model.score(X_test, Y_test))

From above I can infer that Linear Regression model performs very poorly on our dataset.

As we can see the variance is in negative numbers. Which indicates that our model is Poor.

Minimum variance helps our model to be ideal / accurate, Therefore further down I'll implement RandomForest algorithms and check for accuray.

If accuracy increses, then I will pick the model.

Another way we can check accuracy by showing comparison between actual values and predicted values.

Actual_vs_Pred = pd.DataFrame({"Actual Sales" : Y_test, "Predicted Sales": y_pred})
Actual_vs_Pred.head()

sns.regplot(x=y_pred, y=Y_test, line_kws={'color': 'red'})
plt.title('Actual vs. predicted values')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

rfr = RandomForestRegressor(n_estimators = 400, max_depth = 15)
rfr.fit(X_train, Y_train)
Y_pred = rfr.predict(X_test)

print('Accuracy:',rfr.score(X_train,Y_train))
print('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, Y_pred))
print('Mean Squared Error:', metrics.mean_squared_error(Y_test, Y_pred))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))
print('-'*40)
print('Variance score: %.2f' % rfr.score(X_test, Y_test))

Actual_vs_Pred = pd.DataFrame({"Actual Sales" : Y_test, "Predicted Sales": Y_pred})
Actual_vs_Pred.head()

Errors = pd.DataFrame({'errors':round(((abs(Y_pred - Y_test))/Y_test)*100,2)})
Errors.head()

sns.regplot(x=Y_pred, y=Y_test)
plt.title('Actual vs. predicted values')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

So here we can see, we have predicted demand for store 1
 
Further down I've shown a table that shows errors in %, w.r.t the store sales demand.

Errors are not huge to decline our model also the variance is good.

Therefore we accept this Model - RandomForest Regressor.

